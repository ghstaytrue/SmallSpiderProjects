{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "'''\n",
    "使用进程池爬取解析及存储解析结果示例\n",
    "爬取链家网天津二手房成交数据，并保存在'data/lianjia'目录下\n",
    "'''\n",
    "\n",
    "\n",
    "class CrawlProcess(object):\n",
    "    '''\n",
    "    配合进程池进行URL链接爬取及结果解析；\n",
    "    最终通过crawl方法的complete_callback参数进行爬取解析结果回调\n",
    "    '''\n",
    "    def __init__(self):  \n",
    "        self.proxies = {\"http\":\"http://221.14.140.130:80\"}\n",
    "        self.headers = {\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36\"\n",
    "                        \" (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\"}\n",
    "        self.iplist = [{'https':'https://42.51.12.2:808'},\n",
    "                         {'https':'https://182.202.221.101:61234'},\n",
    "                         {'http':'http://117.60.161.136:61234'},\n",
    "                         {'http':'http://182.202.223.41:61234'},\n",
    "                         {'https':'https://1.196.161.170:9999'},\n",
    "                         {'https':'https://124.237.83.14:53281'},\n",
    "                         {'http':'http://125.46.0.62:53281'},\n",
    "                         {'https':'https://120.77.254.116:3128'},\n",
    "                         {'http':'http://218.241.234.48:8080'},\n",
    "                         {'http':'http://203.174.112.13:3128'},\n",
    "                         {'https':'https://222.222.169.60:53281'},\n",
    "                         {'https':'https://202.38.92.100:3128'},\n",
    "                         {'https':'https://118.212.137.135:31288'},\n",
    "                         {'https':'https://139.224.80.139:3128'},\n",
    "                         {'https':'https://122.72.18.35:80'},\n",
    "                         {'https':'https://119.28.152.208:80'},\n",
    "                         {'http':'http://113.200.214.164:9999'},\n",
    "                         {'https':'https://123.161.16.20:9797'},\n",
    "                         {'https':'https://114.215.95.188:3128'},\n",
    "                         {'https':'https://122.72.18.34:80'},\n",
    "                         {'https':'https://125.66.165.155:53281'},\n",
    "                         {'https':'https://122.136.212.132:53281'},\n",
    "                         {'http':'http://113.106.195.98:8088'},\n",
    "                         {'https':'https://101.37.79.125:3128'}]\n",
    "        #伪装浏览器头\n",
    "        self.user_agent_list = [\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n",
    "                                \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\",\n",
    "                                \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\",\n",
    "                                \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\",\n",
    "                                \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n",
    "                                ]\n",
    "\n",
    "    def get(self,url, timeout =5, proxy=None, num_retries=10):\n",
    "        UA = random.choice(self.user_agent_list)  ##从user_agent_list中随机取出一个字符串\n",
    "        headers = {'User-Agent':UA}\n",
    "        if proxy == None:\n",
    "            try:\n",
    "                return requests.get(url, headers=self.headers,timeout=timeout)  ##这样服务器就会以为我们是真的浏览器了\n",
    "            except:  ##如过上面的代码执行报错则执行下面的代码\n",
    "                if num_retries > 0:  ##num_retries是我们限定的重试次数\n",
    "                    time.sleep(3)  ##延迟3秒\n",
    "                    print('获取网页出错，3S后将获取倒数第：', num_retries, '次')\n",
    "                    return get(url, timeout,None,num_retries - 1)  ##调用自身 并将次数减1\n",
    "                else:\n",
    "                    print('开始使用代理')\n",
    "                    time.sleep(3)\n",
    "                    proxy = random.choice(self.iplist)\n",
    "                    return get(url,proxy=proxy)  ##代理不为空的时候\n",
    "        else:\n",
    "            try:\n",
    "                proxy = random.choice(self.iplist) ##将从self.iplist中获取的字符串处理成我们需要的格式（处理了些什么自己看哦，这是基础呢）\n",
    "                return requests.get(url, headers=self.headers, proxies=proxy,timeout=timeout)  ##使用代理获取response\n",
    "            except:\n",
    "                if num_retries > 0:\n",
    "                    time.sleep(3)\n",
    "                    proxy = random.choice(self.iplist)\n",
    "                    print('正在更换代理，3S后将重新获取倒数第', num_retries, '次')\n",
    "                    print('当前代理是：', proxy)\n",
    "                    return get(url, timeout, proxy, num_retries - 1)\n",
    "                else:\n",
    "                    print('代理也不好使了！取消代理')\n",
    "                    return get(url)\n",
    "\n",
    "    def _request_parse_runnable(self, turl):\n",
    "        try:\n",
    "            result = []\n",
    "            retu = {}\n",
    "            isInRoad = {'东丽': ['二号桥街', '月牙河','钢管公司'],\n",
    "                         '北辰': ['双环邨', '武清其他', '杨柳青'],\n",
    "                         '南开': ['大胡同', '鸿顺里街', '南市', '三条石', '铁东路', '天穆镇', '新开河', '小淀镇', '宜兴埠'],\n",
    "                         '塘沽': ['滨海其他','三槐路街','新城镇'],\n",
    "                         '河东': ['靖江路', '万新街', '张贵庄','鲁山道'],\n",
    "                         '河北': ['靖江路', '真理道'],\n",
    "                         '津南': ['大寺', '尖山', '柳林街', '梅江', '新梅江', '小海地','葛沽镇'],\n",
    "                         '西青': ['八里台', '华苑', '梅江', '双口镇', '双港', '体育中心街', '王顶堤', '新梅江', '向阳路','辛口镇','西堤头'],\n",
    "                         '武清': ['静湖']}\n",
    "            url = turl.split('-')[0]\n",
    "            area = turl.split('-')[1]\n",
    "            road = turl.split('-')[2]\n",
    "            isroad = turl.split('-')[3]\n",
    "            count_area = int(turl.split('-')[4])\n",
    "            count_road = int(turl.split('-')[5])\n",
    "            if isroad == \"True\":\n",
    "                rep_road = requests.get(url=url,headers=self.headers,proxies=self.proxies)\n",
    "#                 rep_road = self.get(url=url,proxy=1)\n",
    "                sp_road = BeautifulSoup(rep_road.text,'lxml')\n",
    "                post_area = sp_road.select('.position')[0].select('div')[0].select('a')[count_area]\n",
    "                area = post_area.text    \n",
    "                post_road = sp_road.select('.position')[0].select('div')[0].select('div')[1].select('a')[count_road]\n",
    "                url = 'https://tj.lianjia.com'+post_road['href']\n",
    "                print('A:',url)\n",
    "                road = post_road.text\n",
    "                isroad = 'False'\n",
    "#                 count_road += 1\n",
    "            time.sleep(1)\n",
    "            response = requests.get(url=url,headers=self.headers,proxies=self.proxies)\n",
    "#             response = self.get(url=url,proxy=1)\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "            for info in soup.select('.img'):\n",
    "                result.append(info['href'])\n",
    "            text = str(soup.select('.page-box')[0])\n",
    "            totalPage = int(re.findall(re.compile(r\"totalPage\\\":.*?(\\d+)\"),text)[0])\n",
    "            curPage = int(re.findall(re.compile(r\"curPage\\\":.*?(\\d+)\"),text)[0])\n",
    "            href = url.split('/')\n",
    "            href[5] = '%s%d%s'%('pg',(curPage+1),'ddo21')\n",
    "            url = \"/\".join(href)\n",
    "            if totalPage == curPage:\n",
    "                count_road += 1\n",
    "                while True:\n",
    "                    if len(soup.select('.position')[0].select('div')[0].select('div')[1].select('a')) > count_road:\n",
    "                        if soup.select('.position')[0].select('div')[0].select('a')[count_area].text in list(isInRoad.keys()):\n",
    "                            if soup.select('.position')[0].select('div')[0].select('div')[1].select('a')[count_road].text in isInRoad.get(area):\n",
    "                                count_road += 1\n",
    "                            else:\n",
    "                                break\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                if len(soup.select('.position')[0].select('div')[0].select('div')[1].select('a')) > count_road:\n",
    "                    post_road = soup.select('.position')[0].select('div')[0].select('div')[1].select('a')[count_road]\n",
    "                    url = 'https://tj.lianjia.com'+post_road['href']\n",
    "                    print('B:',url)\n",
    "                    isroad = 'True'\n",
    "                else:\n",
    "                    count_area += 1\n",
    "                    post_area = soup.select('.position')[0].select('div')[0].select('a')[count_area]\n",
    "                    url = 'https://tj.lianjia.com'+post_area['href']\n",
    "                    print('C:',url)\n",
    "                    isroad = 'True'\n",
    "                    count_road = 0\n",
    "            retu['url'] = '%s-%s-%s-%s-%d-%d'%(url,area,road,isroad,count_area,count_road)\n",
    "            retu['urls'] = result\n",
    "        except BaseException as e:\n",
    "            print('connection error.'+str(e))\n",
    "            print(url)\n",
    "            retu = None\n",
    "        return retu\n",
    "\n",
    "    def crawl(self, url, complete_callback, process_pool):\n",
    "        future = process_pool.submit(self._request_parse_runnable, url)\n",
    "        future.add_done_callback(complete_callback)\n",
    "\n",
    "\n",
    "class OutPutProcess(object):\n",
    "    '''\n",
    "    配合进程池对上面爬取解析进程结果进行进程池处理存储；\n",
    "    '''\n",
    "    def _output_runnable(self, crawl_result):\n",
    "        try:\n",
    "            if crawl_result == None:\n",
    "                return crawl_result\n",
    "            save_dir = 'data/lianjia'\n",
    "            if os.path.exists(save_dir) is False:\n",
    "                os.makedirs(save_dir)\n",
    "            save_file = save_dir + os.path.sep + 'LJdata' + '.csv'\n",
    "            if os.path.exists(save_file):\n",
    "                csv = pd.DataFrame.from_dict(crawl_result,orient='index').T\n",
    "                csv.to_csv(save_file,index=False,header=False,mode='a+',encoding=\"gbk\")\n",
    "            else:\n",
    "                csv = pd.DataFrame.from_dict(crawl_result,orient='index').T\n",
    "                csv.to_csv(save_file,index=False,header=True,encoding='gbk')\n",
    "        except Exception as e:\n",
    "            print('save file error.'+str(e))\n",
    "        return crawl_result\n",
    "\n",
    "    def save(self, crawl_result, process_pool):\n",
    "        process_pool.submit(self._output_runnable, crawl_result)\n",
    "\n",
    "class ParserProcess(object):\n",
    "    '''\n",
    "    获取内容类，获取房子详细信息\n",
    "    '''\n",
    "    def __init__(self):  \n",
    "        self.proxies = {\"http\":\"http://221.14.140.130:80\"}\n",
    "        self.headers = {\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36\"\n",
    "                        \" (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\"}\n",
    "    \n",
    "    def _content_runnable(self,url):\n",
    "        try:\n",
    "            content_result = {}\n",
    "            time.sleep(1)\n",
    "            area = url.split('-')[0]\n",
    "            road = url.split('-')[1]\n",
    "            url = url.split('-')[2]\n",
    "            response = requests.get(url=url,headers=self.headers,proxies=self.proxies)\n",
    "#             response = self.get(url=url,proxy=1)\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "            content_result['地区'] = area\n",
    "            content_result['街道'] = road\n",
    "            content_result['小区名'] = soup.select('.index_h1')[0].text.split(' ')[0]\n",
    "            for roominfo in soup.select('.content'):\n",
    "                for li in roominfo.select('li'):\n",
    "                    if li.select('.label')[0].text == '建筑面积':\n",
    "                        content_result[li.select('.label')[0].text+'(平方米)']=li.text.strip(li.select('.label')[0].text).strip().strip('㎡')\n",
    "                    elif li.select('.label')[0].text == '套内面积':\n",
    "                        if li.text.strip(li.select('.label')[0].text).strip() == '暂无数据':\n",
    "                            content_result[li.select('.label')[0].text+'(平方米)']=li.text.strip(li.select('.label')[0].text).strip()\n",
    "                        else:\n",
    "                            content_result[li.select('.label')[0].text+'(平方米)']=li.text.strip(li.select('.label')[0].text).strip().strip('㎡')\n",
    "                    elif li.select('.label')[0].text == '链家编号':\n",
    "                        pass\n",
    "                    elif li.select('.label')[0].text == '产权年限':\n",
    "                        content_result[li.select('.label')[0].text+\"(年)\"]=li.text.strip(li.select('.label')[0].text).strip().strip('年')\n",
    "                    elif li.select('.label')[0].text == '所在楼层':\n",
    "                        content_result[li.select('.label')[0].text] = li.text.strip(li.select('.label')[0].text).strip().split('(')[0].strip()\n",
    "                        content_result['共几层楼'] = int(re.findall(re.compile(r\"\\d+\"),li.text.strip(li.select('.label')[0].text).strip())[0])\n",
    "                    else:\n",
    "                        content_result[li.select('.label')[0].text]=li.text.strip(li.select('.label')[0].text).strip()\n",
    "            content_result['总价(万)'] = soup.select('.record_price')[0].text.strip('万')\n",
    "            content_result['单价(元/平方米)'] = soup.select('.record_detail')[0].text.split(',')[0].strip('单价').strip('元/平')\n",
    "            content_result['成交日期'] = soup.select('.record_detail')[0].text.split(',')[2]\n",
    "            content_result['挂牌价格(万)'] = soup.select('.msg')[0].select('label')[0].text\n",
    "            content_result['成交周期(天)'] = soup.select('.msg')[0].select('label')[1].text\n",
    "            content_result['调价次数(次)'] = soup.select('.msg')[0].select('label')[2].text\n",
    "            content_result['关注人数(人)'] = soup.select('.msg')[0].select('label')[4].text\n",
    "            content_result['浏览次数(次)'] = soup.select('.msg')[0].select('label')[5].text\n",
    "        except Exception as e:\n",
    "            print('parser error.'+str(e))\n",
    "            print(url)\n",
    "            dict_url = {}\n",
    "            dict_url['地区'] = area\n",
    "            dict_url['街道'] = road\n",
    "            dict_url['ErrorUrl'] = url\n",
    "            try:\n",
    "                save_dir = 'data/lianjia'\n",
    "                if os.path.exists(save_dir) is False:\n",
    "                    os.makedirs(save_dir)\n",
    "                save_file = save_dir + os.path.sep + 'ErrorUrls' + '.csv'\n",
    "                if os.path.exists(save_file):\n",
    "                    csv = pd.DataFrame.from_dict(dict_url,orient='index').T\n",
    "                    csv.to_csv(save_file,index=False,header=False,mode='a+',encoding=\"gbk\")\n",
    "                else:\n",
    "                    csv = pd.DataFrame.from_dict(dict_url,orient='index').T\n",
    "                    csv.to_csv(save_file,index=False,header=True,encoding='gbk')\n",
    "            except Exception as e:\n",
    "                print('save error url error.'+str(e))\n",
    "        return content_result\n",
    "    \n",
    "    def content(self,url, complete_callback, process_pool):\n",
    "        future = process_pool.submit(self._content_runnable, url)\n",
    "        future.add_done_callback(complete_callback)\n",
    "    \n",
    "class CrawlManager(object):\n",
    "    '''\n",
    "    爬虫管理类，进程池负责统一管理调度爬取解析及存储进程\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.crawl = CrawlProcess()\n",
    "        self.parser = ParserProcess()\n",
    "        self.output = OutPutProcess()\n",
    "        self.crawl_pool = ProcessPoolExecutor(max_workers=8)\n",
    "        self.crawl_deep = 2322   #爬取深度\n",
    "        self.crawl_cur_count = 0\n",
    "\n",
    "    def _crawl_future_callback(self, crawl_url_future):\n",
    "        try:\n",
    "            res = crawl_url_future.result()\n",
    "            urls = res.get('urls')\n",
    "            url = res.get('url')\n",
    "            area = url.split('-')[1]\n",
    "            road = url.split('-')[2]\n",
    "            for href in urls:\n",
    "                href = '%s-%s-%s'%(area,road,href)\n",
    "                self.parser.content(href,self._parser_callback,self.crawl_pool)\n",
    "            self.start_runner(url)\n",
    "        except Exception as e:\n",
    "            print('Run crawl url future process error. '+str(e))\n",
    "            print(url)\n",
    "    \n",
    "    def _parser_callback(self,parser_content_future):\n",
    "        try:\n",
    "            data = parser_content_future.result()\n",
    "            self.output.save(data,self.crawl_pool)\n",
    "        except Exception as e:\n",
    "            print('Run parser contetn future process error.'+str(e))\n",
    "\n",
    "    def start_runner(self, url):\n",
    "        if self.crawl_cur_count > self.crawl_deep:\n",
    "            print('结束！')\n",
    "            return\n",
    "        print(self.crawl_cur_count+1)\n",
    "        self.crawl_cur_count += 1\n",
    "        self.crawl.crawl(url, self._crawl_future_callback, self.crawl_pool)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root_url = '%s-%s-%s-%s-%d-%d'%('https://tj.lianjia.com/chengjiao/heping/ddo21/','','','True',0,0)\n",
    "    CrawlManager().start_runner(root_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
